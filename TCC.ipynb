{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sweetviz as sv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from supervised.automl import AutoML\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "source": [
    "# Grupo\n",
    "\n",
    "Renato Aurélio Fernandes Totti - 171586\n",
    "\n",
    "Thiago Pereira Correa - 171085\n",
    "\n",
    "André Chierighini - 171340\n",
    "\n",
    "Ciro Guimarães - 152539\n",
    "\n",
    "Camila de Carvalho Mendes - 171660\n",
    "\n",
    "Letícia Vigna Modenese Silva - 171869\n",
    "\n",
    "---\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Leitura dos dados;\n",
    "\n",
    "Primeiro é analizado os dados brutos:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>284807.000000</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>...</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>284807.000000</td>\n      <td>284807.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>94813.859575</td>\n      <td>1.168375e-15</td>\n      <td>3.416908e-16</td>\n      <td>-1.379537e-15</td>\n      <td>2.074095e-15</td>\n      <td>9.604066e-16</td>\n      <td>1.487313e-15</td>\n      <td>-5.556467e-16</td>\n      <td>1.213481e-16</td>\n      <td>-2.406331e-15</td>\n      <td>...</td>\n      <td>1.654067e-16</td>\n      <td>-3.568593e-16</td>\n      <td>2.578648e-16</td>\n      <td>4.473266e-15</td>\n      <td>5.340915e-16</td>\n      <td>1.683437e-15</td>\n      <td>-3.660091e-16</td>\n      <td>-1.227390e-16</td>\n      <td>88.349619</td>\n      <td>0.001727</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>47488.145955</td>\n      <td>1.958696e+00</td>\n      <td>1.651309e+00</td>\n      <td>1.516255e+00</td>\n      <td>1.415869e+00</td>\n      <td>1.380247e+00</td>\n      <td>1.332271e+00</td>\n      <td>1.237094e+00</td>\n      <td>1.194353e+00</td>\n      <td>1.098632e+00</td>\n      <td>...</td>\n      <td>7.345240e-01</td>\n      <td>7.257016e-01</td>\n      <td>6.244603e-01</td>\n      <td>6.056471e-01</td>\n      <td>5.212781e-01</td>\n      <td>4.822270e-01</td>\n      <td>4.036325e-01</td>\n      <td>3.300833e-01</td>\n      <td>250.120109</td>\n      <td>0.041527</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-5.640751e+01</td>\n      <td>-7.271573e+01</td>\n      <td>-4.832559e+01</td>\n      <td>-5.683171e+00</td>\n      <td>-1.137433e+02</td>\n      <td>-2.616051e+01</td>\n      <td>-4.355724e+01</td>\n      <td>-7.321672e+01</td>\n      <td>-1.343407e+01</td>\n      <td>...</td>\n      <td>-3.483038e+01</td>\n      <td>-1.093314e+01</td>\n      <td>-4.480774e+01</td>\n      <td>-2.836627e+00</td>\n      <td>-1.029540e+01</td>\n      <td>-2.604551e+00</td>\n      <td>-2.256568e+01</td>\n      <td>-1.543008e+01</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>54201.500000</td>\n      <td>-9.203734e-01</td>\n      <td>-5.985499e-01</td>\n      <td>-8.903648e-01</td>\n      <td>-8.486401e-01</td>\n      <td>-6.915971e-01</td>\n      <td>-7.682956e-01</td>\n      <td>-5.540759e-01</td>\n      <td>-2.086297e-01</td>\n      <td>-6.430976e-01</td>\n      <td>...</td>\n      <td>-2.283949e-01</td>\n      <td>-5.423504e-01</td>\n      <td>-1.618463e-01</td>\n      <td>-3.545861e-01</td>\n      <td>-3.171451e-01</td>\n      <td>-3.269839e-01</td>\n      <td>-7.083953e-02</td>\n      <td>-5.295979e-02</td>\n      <td>5.600000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>84692.000000</td>\n      <td>1.810880e-02</td>\n      <td>6.548556e-02</td>\n      <td>1.798463e-01</td>\n      <td>-1.984653e-02</td>\n      <td>-5.433583e-02</td>\n      <td>-2.741871e-01</td>\n      <td>4.010308e-02</td>\n      <td>2.235804e-02</td>\n      <td>-5.142873e-02</td>\n      <td>...</td>\n      <td>-2.945017e-02</td>\n      <td>6.781943e-03</td>\n      <td>-1.119293e-02</td>\n      <td>4.097606e-02</td>\n      <td>1.659350e-02</td>\n      <td>-5.213911e-02</td>\n      <td>1.342146e-03</td>\n      <td>1.124383e-02</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>139320.500000</td>\n      <td>1.315642e+00</td>\n      <td>8.037239e-01</td>\n      <td>1.027196e+00</td>\n      <td>7.433413e-01</td>\n      <td>6.119264e-01</td>\n      <td>3.985649e-01</td>\n      <td>5.704361e-01</td>\n      <td>3.273459e-01</td>\n      <td>5.971390e-01</td>\n      <td>...</td>\n      <td>1.863772e-01</td>\n      <td>5.285536e-01</td>\n      <td>1.476421e-01</td>\n      <td>4.395266e-01</td>\n      <td>3.507156e-01</td>\n      <td>2.409522e-01</td>\n      <td>9.104512e-02</td>\n      <td>7.827995e-02</td>\n      <td>77.165000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>172792.000000</td>\n      <td>2.454930e+00</td>\n      <td>2.205773e+01</td>\n      <td>9.382558e+00</td>\n      <td>1.687534e+01</td>\n      <td>3.480167e+01</td>\n      <td>7.330163e+01</td>\n      <td>1.205895e+02</td>\n      <td>2.000721e+01</td>\n      <td>1.559499e+01</td>\n      <td>...</td>\n      <td>2.720284e+01</td>\n      <td>1.050309e+01</td>\n      <td>2.252841e+01</td>\n      <td>4.584549e+00</td>\n      <td>7.519589e+00</td>\n      <td>3.517346e+00</td>\n      <td>3.161220e+01</td>\n      <td>3.384781e+01</td>\n      <td>25691.160000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_dataset = pd.read_csv('./data/creditcard.csv', sep=',', index_col=None) \n",
    "\n",
    "\n",
    "print(df_dataset.shape)\n",
    "\n",
    "\n",
    "df_dataset.describe()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "source": [
    "# Análise exploratória;"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Aqui pode-se perceber que como os dadosa v1 ~ v28 são dados anonimizados das transações é difícil encontrar valores condizentes, os dados de Time e Amount foram removidos por apresentar valores discrepantes quando comparados aos outros"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datasetWithouAmountAndTime = df_dataset\n",
    "df_datasetWithouAmountAndTime =df_dataset.drop(['Time','Amount'], axis = 1)\n",
    "df_datasetWithouAmountAndTime.plot.box(figsize=(15,7))\n"
   ]
  },
  {
   "source": [
    "Box plot de tempo mostra um valor adequado já que é a contagem de tempo desde o início"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['Time'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['Amount'].plot.box()"
   ]
  },
  {
   "source": [
    "Após toda analise inicial, é feita a primeira limpeza onde é removido duplicados, depois gerado uma nova analise com base no atributo \"Class\", o qual indica qual linha é fraude ou não.\n",
    "\n",
    "Aqui utiliza-se a biblioteca chamada sweetviz, que gera um relatório com as informações mais relevantes para a decisão da utilização dos dados do dataset separando os dados que mais se correlacionam com o `target_feat`, nesse caso `Class`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "df_dataset = pd.read_csv('./data/creditcard.csv', sep=',', index_col=None) \n",
    "df_dataset=df_dataset.drop_duplicates()\n",
    "my_report = sv.analyze(df_dataset,target_feat='Class')\n",
    "my_report.show_notebook()"
   ]
  },
  {
   "source": [
    "Um segundo relátorio gerado com o sweetivz, agora com os dados separados, demonstra a distribuição das classes de fraudes dentro do data set comparada ás transações legítimas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv('./data/creditcard.csv', sep=',', index_col=None) \n",
    "df_dataset=df_dataset.drop_duplicates()\n",
    "df_dataset_fraude = df_dataset.loc[df_dataset['Class']==1]\n",
    "df_dataset_legitmo = df_dataset.loc[df_dataset['Class']==0]\n",
    "my_report = sv.compare([df_dataset_legitmo,'Legítimo'],[df_dataset_fraude,'Fraude'])\n",
    "my_report.show_notebook()"
   ]
  },
  {
   "source": [
    "É necessário pontuar que como a maioria das colunas são dados anonimizados é de dificil leitura como por exemplo, os dados clássicos da íris setosa, pois apresentam extrema variação por nao se tratar de um dataset \"orgânico\" e sim um dataset de amostras coletadas de comportamento humano, sendo assim, a análise exploratória nos diz que esse data set pode contem transações fraudulentas de extrema dificuldade de classificação caso elas se assemelhem ao comportamento de uma pessoa realizando uma transação legítima."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Pré processamento de dados\n",
    "\n",
    "Feita a analise a partir da classe foi feita uma seleção das colunas que tem uma maior relação no que diz sobre a linha ser fraude ou não, pela matriz de correlação. O sweetviz também facilita a decisão das colunas mais relevantes junto a aba de Class.\n",
    "\n",
    "Vale notar que as colunas e linhas já estavam apropriadas ao uso desse programa, portanto não foi necessária a limpeza profunda. Os relatórios gerados demonstram que nenhum dado estava faltante.\n",
    "\n",
    "Para verificar que a escolha de colunas nao afetou os dados é necessário gerar outro report."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv('./data/creditcard.csv', sep=',', index_col=None) \n",
    "df_dataset=df_dataset[['Amount','V17','V14','V12','V10','V16','V3','V7','V11','V4','V18','V1','Class']]\n",
    "my_report = sv.analyze(df_dataset,target_feat='Class')\n",
    "my_report.show_notebook()"
   ]
  },
  {
   "source": [
    "É possível perceber que a redução de colunas gerou linhas com dados idênticos, portanto é importante remover dados duplicatos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv('./data/creditcard.csv', sep=',', index_col=None) \n",
    "df_dataset=df_dataset[['Amount','V17','V14','V12','V10','V16','V3','V7','V11','V4','V18','V1','Class']]\n",
    "#removendo dados duplicatos\n",
    "df_dataset=df_dataset.drop_duplicates()\n",
    "my_report = sv.analyze(df_dataset,target_feat='Class')\n",
    "my_report.show_notebook()"
   ]
  },
  {
   "source": [
    "# Construção e validação do modelo;\n",
    "Agora com os dados limpos, é feito o treinamento de varios modelos diferentes e após isso é gerado a pontuação(score) de cada modelo de teste treinado.\n",
    "\n",
    "A validação utilizada foi a validação cruzada com k=10, para manter o rigor estatístico e evitar bias dos modelos gerados\n",
    "\n",
    "Para treino foi utilizado um split de 75% e 25% para testes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This model has already been fitted. You can use predict methods or select a new 'results_path' for a new 'fit()'.\nThis model has already been fitted. You can use predict methods or select a new 'results_path' for a new 'fit()'.\nThis model has already been fitted. You can use predict methods or select a new 'results_path' for a new 'fit()'.\nThis model has already been fitted. You can use predict methods or select a new 'results_path' for a new 'fit()'.\nThis model has already been fitted. You can use predict methods or select a new 'results_path' for a new 'fit()'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelos=['Random Forest','Xgboost','Neural Network','Decision Tree','CatBoost']\n",
    "\n",
    "df_dataset = pd.read_csv('./data/creditcard.csv', sep=',', index_col=None) \n",
    "df_dataset=df_dataset[['Amount','V17','V14','V12','V10','V16','V3','V7','V11','V4','V18','V1','Class']]\n",
    "#removendo dados duplicatos\n",
    "df_dataset=df_dataset.drop_duplicates()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_dataset[df_dataset.columns[:-1]], df_dataset[\"Class\"], test_size=0.25\n",
    ")\n",
    "for modelo in modelos:\n",
    "    automl = AutoML(results_path=f'AC2_{modelo}',algorithms=[modelo],eval_metric='f1',explain_level=1,validation_strategy={\"validation_type\": \"kfold\",\"k_folds\": 10, \"shuffle\": True,\"random_seed\": 1997},total_time_limit=60*30)\n",
    "    automl.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "source": [
    "---\n",
    "\n",
    "# Discussão dos resultados.\n",
    "\n",
    "Para entendermos os modelos primeiramente é necessário saber quantas fraudes existem no dataset de teste"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fraudes contidas no set de testes:103\n"
     ]
    }
   ],
   "source": [
    "totFrauds=len(y_test.loc[y_test==1])\n",
    "print(f'Fraudes contidas no set de testes:{totFrauds}')"
   ]
  },
  {
   "source": [
    "Também é preciso calcular a score de cada modelo perante ao set de testes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Modelo:Random Forest score:99.94%\n",
      "Modelo:Xgboost score:99.97%\n",
      "Modelo:Neural Network score:99.94%\n",
      "Modelo:Decision Tree score:99.93%\n",
      "Modelo:CatBoost score:99.96%\n"
     ]
    }
   ],
   "source": [
    "for modelo in modelos:\n",
    "    automl = AutoML(results_path=f'AC2_{modelo}')\n",
    "    score = automl.score(X_test,y_test)\n",
    "    print(f\"Modelo:{modelo} score:{round(score*100,2)}%\")"
   ]
  },
  {
   "source": [
    "modelos=['Random Forest','Xgboost','Neural Network','Decision Tree','CatBoost']\n",
    "modelNumber=0\n",
    "\n",
    "errorsHigh=[0,0,0,0,0]\n",
    "dataHigh=[[],[],[],[],[]]\n",
    "\n",
    "errorslow=[0,0,0,0,0]\n",
    "dataLow=[[],[],[],[],[]]\n",
    "for modelo in modelos:\n",
    "    automl = AutoML(results_path=f'AC2_{modelo}')\n",
    "    pred=automl.predict(X_test)\n",
    "    i=0\n",
    "    errors=0\n",
    "    for y in y_test:\n",
    "        if(y==1 and pred[i]==0):\n",
    "            amt=X_test.loc[i,'Amount']\n",
    "            if(amt >= 100):\n",
    "                dataHigh[modelNumber].append(amt)\n",
    "                errorsHigh[modelNumber]+=1\n",
    "            else:\n",
    "                dataLow[modelNumber].append(amt)\n",
    "                errorslow[modelNumber]+=1\n",
    "            errors+=1\n",
    "        i+=1\n",
    "    modelNumber+=1\n",
    "\n",
    "    print(f'Modelo:{modelo} | Falsos negativos:{errors} | Porcentagem de falsos negativos:{round((errors/totFrauds)*100,2)}%')\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 43,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Modelo:Random Forest | Falsos negativos:25 | Porcentagem de falsos negativos:24.27%\n",
      "Modelo:Xgboost | Falsos negativos:18 | Porcentagem de falsos negativos:17.48%\n",
      "Modelo:Neural Network | Falsos negativos:20 | Porcentagem de falsos negativos:19.42%\n",
      "Modelo:Decision Tree | Falsos negativos:29 | Porcentagem de falsos negativos:28.16%\n",
      "Modelo:CatBoost | Falsos negativos:18 | Porcentagem de falsos negativos:17.48%\n"
     ]
    }
   ]
  },
  {
   "source": [
    "É possível perceber que os falsos negativos giram em torno de 20-28% de todas as predições porém esse valor sozinho nao quer dizer toda a análise necessária. É muito interessante analisar os valores desse falsos positivos.\n",
    "\n",
    "Foi escolhido um valor arbitrário de 100 euros para representar fraudes de valor considerável"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tabela de erros com mais de 100 euros\n      Random Forest    Xgboost    Neural Network    Decision Tree    CatBoost\n--  ---------------  ---------  ----------------  ---------------  ----------\n 0          1096.99    1096.99           1096.99          1096.99     1096.99\n 1           519.9      519.9             519.9            105.89      519.9\n 2           208.58     208.58            208.58           519.9       208.58\n 3           129        129               129              208.58      129\n 4           104.81     104.81            104.81           129         104.81\n 5           634.3      634.3             634.3            104.81      634.3\n 6           247.86     105.99            105.99           634.3       105.99\n 7           105.99     237.26            311.91          1809.68      237.26\n 8           237.26     311.91            nan              105.99      311.91\n 9           311.91     nan               nan              311.91      nan\n"
     ]
    }
   ],
   "source": [
    "print('Tabela de erros com mais de 100 euros')\n",
    "\n",
    "#transposição da tabela para formatar o print\n",
    "dataHigh = pd.DataFrame(dataHigh).T\n",
    "print(tabulate(dataHigh,headers=modelos))"
   ]
  },
  {
   "source": [
    "Como é possível perceber, o modelo de árvore de decisão teve o menor desempenho contendo o maior classificando uma fraude de 1800 euros como uma transação legítima. \n",
    "Porém as várias vezes que o set de teste foi mudado a floresta randômica tambémfoi sucetivel a erros do tipo.\n",
    "\n",
    "\n",
    "É interessante notar como alguns dos valores se repetem dentro dos vários modelos, evidenciando uma possivel similaridade com transações legitimas. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tabela de erros com menos de 100 euros\n      Random Forest    Xgboost    Neural Network    Decision Tree    CatBoost\n--  ---------------  ---------  ----------------  ---------------  ----------\n 0             1          1                 1                2           1\n 1             1          1                 1                1           1\n 2             1          8                 1                1           8\n 3             8          2.47              8                1           0\n 4             0          3.14              0                8           2.47\n 5             2.47       0.76              2.47             0           3.14\n 6            25          3.76              3.14             2.47        0.76\n 7             3.14      42.53              0.76            25          11.39\n 8             0.76     nan                 3.76             3.14        3.76\n 9             1        nan                 0.68             0.76        0.68\n10             3.76     nan                42.53             1          42.53\n11             0.68     nan                 1                3.76        1\n12            42.53     nan                 1                0.68      nan\n13             1        nan               nan               42.53      nan\n14             1        nan               nan                1         nan\n15           nan        nan               nan                1         nan\n"
     ]
    }
   ],
   "source": [
    "print('Tabela de erros com menos de 100 euros')\n",
    "\n",
    "#transposição da tabela para formatar o print\n",
    "dataLow = pd.DataFrame(dataLow).T\n",
    "print(tabulate(dataLow,headers=modelos))"
   ]
  },
  {
   "source": [
    "## Matrizes de confusão e outras métricas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## CatBoost\n",
    "\n",
    "\n",
    "## Metric details\n",
    "|           |      score |     threshold |\n",
    "|:----------|-----------:|--------------:|\n",
    "| logloss   | 0.00309474 | nan           |\n",
    "| auc       | 0.96851    | nan           |\n",
    "| f1        | 0.849612   |   0.474383    |\n",
    "| accuracy  | 0.999531   |   0.474383    |\n",
    "| precision | 0.931973   |   0.474383    |\n",
    "| recall    | 1          |   1.81519e-05 |\n",
    "| mcc       | 0.852726   |   0.474383    |\n",
    "\n",
    "\n",
    "## Confusion matrix (at threshold=0.474383)\n",
    "|                     |   Predicted as negative |   Predicted as positive |\n",
    "|:--------------------|------------------------:|------------------------:|\n",
    "| Labeled as negative |                  206376 |                      20 |\n",
    "| Labeled as positive |                      77 |                     274 |\n",
    "\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Decision Tree\n",
    "\n",
    "\n",
    "## Metric details\n",
    "|           |      score |   threshold |\n",
    "|:----------|-----------:|------------:|\n",
    "| logloss   | 0.00381487 |  nan        |\n",
    "| auc       | 0.889383   |  nan        |\n",
    "| f1        | 0.779874   |    0.443866 |\n",
    "| accuracy  | 0.999323   |    0.443866 |\n",
    "| precision | 0.852234   |    0.443866 |\n",
    "| recall    | 0.994203   |    0        |\n",
    "| mcc       | 0.782371   |    0.443866 |\n",
    "\n",
    "\n",
    "## Confusion matrix (at threshold=0.443866)\n",
    "|                     |   Predicted as negative |   Predicted as positive |\n",
    "|:--------------------|------------------------:|------------------------:|\n",
    "| Labeled as negative |                  206359 |                      43 |\n",
    "| Labeled as positive |                      97 |                     248 |\n",
    "\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Neural Network\n",
    "\n",
    "\n",
    "## Metric details\n",
    "|           |      score |     threshold |\n",
    "|:----------|-----------:|--------------:|\n",
    "| logloss   | 0.00386043 | nan           |\n",
    "| auc       | 0.950215   | nan           |\n",
    "| f1        | 0.822653   |   0.452456    |\n",
    "| accuracy  | 0.999424   |   0.452456    |\n",
    "| precision | 0.846626   |   0.452456    |\n",
    "| recall    | 1          |   3.8731e-153 |\n",
    "| mcc       | 0.822696   |   0.452456    |\n",
    "\n",
    "\n",
    "## Confusion matrix (at threshold=0.452456)\n",
    "|                     |   Predicted as negative |   Predicted as positive |\n",
    "|:--------------------|------------------------:|------------------------:|\n",
    "| Labeled as negative |                  206352 |                      50 |\n",
    "| Labeled as positive |                      69 |                     276 |\n",
    "\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Random Forest\n",
    "\n",
    "## Metric details\n",
    "|           |      score |     threshold |\n",
    "|:----------|-----------:|--------------:|\n",
    "| logloss   | 0.00337719 | nan           |\n",
    "| auc       | 0.914741   | nan           |\n",
    "| f1        | 0.799387   |   0.459631    |\n",
    "| accuracy  | 0.999366   |   0.459631    |\n",
    "| precision | 0.847403   |   0.459631    |\n",
    "| recall    | 1          |   0.000208493 |\n",
    "| mcc       | 0.800361   |   0.459631    |\n",
    "\n",
    "\n",
    "## Confusion matrix (at threshold=0.459631)\n",
    "|                     |   Predicted as negative |   Predicted as positive |\n",
    "|:--------------------|------------------------:|------------------------:|\n",
    "| Labeled as negative |                  206355 |                      47 |\n",
    "| Labeled as positive |                      84 |                     261 |\n",
    "\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Extreme Gradient Boosting (Xgboost)\n",
    "\n",
    "## Metric details\n",
    "|           |    score |   threshold |\n",
    "|:----------|---------:|------------:|\n",
    "| logloss   | 0.133929 | nan         |\n",
    "| auc       | 0.921456 | nan         |\n",
    "| f1        | 0.853168 |   0.456902  |\n",
    "| accuracy  | 0.999541 |   0.456902  |\n",
    "| precision | 0.913907 |   0.456902  |\n",
    "| recall    | 1        |   0.0130897 |\n",
    "| mcc       | 0.854835 |   0.456902  |\n",
    "\n",
    "\n",
    "## Confusion matrix (at threshold=0.456902)\n",
    "|                     |   Predicted as negative |   Predicted as positive |\n",
    "|:--------------------|------------------------:|------------------------:|\n",
    "| Labeled as negative |                  206376 |                      26 |\n",
    "| Labeled as positive |                      69 |                     276 |\n",
    "\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "É interessante notar as altas taxas de precisão e acurácia para os modelos principalmente os modelos de xgboost catboost e rede neural. Porém é possivel notar a alta taxa de falsos negativos dentro da matriz de confusão, como citado no capítulo de análise exploratória, fraudes que se comportam de modo homólogo á uma transação legítima é extremamente difícil discernir as duas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Conclusão\n",
    "\n",
    "O Extreme Grandient Boost (xgboost) sempre se mostrou o melhor modelo quanto a acurária (em raras vezes igual ou menor ao CatBoost). Totdavia a rede neural sempre teve taxa de sucesso em não criar falsos negativos com transações de alto valor. Por mais que a acurácia dela seja menor, provavelmente se msotra como a mais interessante a ser aplicada para a resolução do problema.\n",
    "\n",
    "Vale citar que os falsos negativos apresentados pelos modelos quase sempre são relacionados as mesmas transações evidenciando a dificuldade em modelar e classificar comportamento humano dentro de um problema de IA."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}